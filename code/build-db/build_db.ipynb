{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import avro\n",
    "from avro.datafile import DataFileWriter, DataFileReader\n",
    "from avro.io import DatumWriter, DatumReader\n",
    "import copy\n",
    "import json\n",
    "from io import BytesIO, StringIO\n",
    "import os\n",
    "import csv\n",
    "import datetime\n",
    "import fastavro\n",
    "import logging\n",
    "import mysql.connector\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "import pandavro as pdx\n",
    "import pickle\n",
    "import psycopg\n",
    "import psycopg2\n",
    "import sqlalchemy\n",
    "import psycopg_binary\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test (obsolete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), \"requirement-durations\")\n",
    "df = pd.read_csv(os.path.join(data_dir, '000000000208.csv'),\n",
    "                 header=0,\n",
    "                 sep=';',\n",
    "                 on_bad_lines='skip',\n",
    "                 usecols=['System', 'Name', 'Version', 'RequirementName', 'HighestMatchingVersion'],\n",
    "                 )\n",
    "df.columns = ['SystemName', 'FromPackageName', 'FromVersion', 'ToPackageName', 'ToVersion']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.dropna()\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.drop_duplicates()\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"mysql+pymysql://{user}:{pw}@localhost/{db}\"\n",
    "                       .format(user=\"root\",\n",
    "                               pw=\"Shimul@12345\",\n",
    "                               db=\"secmet-test\"))\n",
    "with engine.begin() as connection:\n",
    "    df3.to_sql(con=connection, name='Relations', if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POSTGRES (obsolete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), \"package-versions\")\n",
    "df = pd.read_csv(os.path.join(data_dir, '000000005315.csv'),\n",
    "                 header=0,\n",
    "                 sep=';',\n",
    "                 on_bad_lines='skip',\n",
    "                 usecols=['System', 'Name', 'Version', 'UpstreamPublishedAt'],\n",
    "                 )\n",
    "df.columns = ['SystemName', 'PackageName', 'Version', 'ReleaseDate']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.dropna()\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.drop_duplicates()\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install psycopg: https://stackoverflow.com/a/75664040/3450691\n",
    "engine = sqlalchemy.create_engine(\"postgresql+psycopg://{user}:{pw}@localhost/{db}\"\n",
    "                       .format(user=\"postgres\",\n",
    "                               pw=\"shimul\",\n",
    "                               db=\"postgres\"))\n",
    "with engine.begin() as connection:\n",
    "    df3.to_sql(con=connection, name='VersionInfo', if_exists='append', index=False)\n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sqlalchemy.create_engine(\"postgresql+psycopg://{user}:{pw}@localhost:{port}/{db}\"\n",
    "                       .format(user=\"postgres\",\n",
    "                               pw=\"shimul\",\n",
    "                               port=\"5432\",\n",
    "                               db=\"postgres\"))\n",
    "with engine.begin() as connection:\n",
    "    print (connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Relations in POSTGRES (obsolete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), \"requirement-durations\")\n",
    "df = pd.read_csv(os.path.join(data_dir, '000000000208.csv'),\n",
    "                 header=0,\n",
    "                 sep=';',\n",
    "                 on_bad_lines='skip',\n",
    "                 usecols=['System', 'Name', 'Version', 'RequirementName', 'RequirementVersion', 'HighestMatchingVersion', 'HighestAvailableRelease', 'IntervalStart', 'IntervalEnd', 'IsOutOfDate', 'IsRegular', 'Warnings'],\n",
    "                )\n",
    "df.columns = ['system_name', 'from_package_name', 'from_version', 'to_package_name', 'actual_requirement', 'to_version', 'to_package_highest_available_release', 'interval_start', 'interval_end', 'is_out_of_date', 'is_regular', 'warnings']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['system_name', 'from_package_name', 'from_version', 'to_package_name', 'to_version'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the rowns where 'warnings' is not NULL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[df['warnings'].notna()]\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install psycopg: https://stackoverflow.com/a/75664040/3450691\n",
    "engine = sqlalchemy.create_engine(\"postgresql+psycopg://{user}:{pw}@localhost/{db}\"\n",
    "                       .format(user=\"postgres\",\n",
    "                               pw=\"shimul\",\n",
    "                               db=\"postgres\"))\n",
    "with engine.begin() as connection:\n",
    "    df.to_sql(con=connection,\n",
    "                      name='relations',\n",
    "                      if_exists='append',\n",
    "                      index=False,\n",
    "                      dtype={\n",
    "                                'system_name': sqlalchemy.types.VARCHAR,\n",
    "                                'from_package_name': sqlalchemy.types.VARCHAR,\n",
    "                                'from_version': sqlalchemy.types.VARCHAR,\n",
    "                                'to_package_name': sqlalchemy.types.VARCHAR,\n",
    "                                'actual_requirement': sqlalchemy.types.VARCHAR,\n",
    "                                'to_version': sqlalchemy.types.VARCHAR,\n",
    "                                'to_package_highest_available_release': sqlalchemy.types.VARCHAR,\n",
    "                                'interval_start': sqlalchemy.types.TIMESTAMP,\n",
    "                                'interval_end': sqlalchemy.types.TIMESTAMP,\n",
    "                                'is_out_of_date': sqlalchemy.types.BOOLEAN,\n",
    "                                'is_regular': sqlalchemy.types.BOOLEAN,\n",
    "                                'warnings': sqlalchemy.types.VARCHAR\n",
    "                            })\n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VersionInfo populate in POSTGRES (obsolete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def versioninfo_populate_postgres():\n",
    "    engine = sqlalchemy.create_engine(\"postgresql+psycopg://{user}:{pw}@localhost/{db}\"\n",
    "                       .format(user=\"postgres\",\n",
    "                               pw=\"shimul\",\n",
    "                               db=\"postgres\"))\n",
    "    data_dir = os.path.join(\"/Users/imranur/Research/data-shared-from-deps-dev\", \"package-versions\")\n",
    "    total_number_of_rows = 0\n",
    "    for csv_file in os.listdir(data_dir):\n",
    "        df = pd.read_csv(os.path.join(data_dir, csv_file),\n",
    "                        header=0,\n",
    "                        sep=';',\n",
    "                        on_bad_lines='skip',\n",
    "                        usecols=['System', 'Name', 'Version', 'UpstreamPublishedAt'],\n",
    "                        )\n",
    "        df.columns = ['system_name', 'package_name', 'version_name', 'release_date']\n",
    "        df = df.dropna()\n",
    "        df = df.drop_duplicates()\n",
    "        with engine.begin() as connection:\n",
    "            df.to_sql(con=connection,\n",
    "                      name='versioninfo',\n",
    "                      if_exists='append',\n",
    "                      index=False,\n",
    "                      dtype={\n",
    "                                'system_name': sqlalchemy.types.VARCHAR,\n",
    "                                'package_name': sqlalchemy.types.VARCHAR,\n",
    "                                'version_name': sqlalchemy.types.VARCHAR,\n",
    "                                'release_date': sqlalchemy.types.TIMESTAMP,\n",
    "                            })\n",
    "        rows = df.shape[0]\n",
    "        total_number_of_rows += rows\n",
    "        print(f\"wrote data from file: {csv_file}, rows: {rows}, total_rows: {total_number_of_rows}\")\n",
    "\n",
    "\n",
    "versioninfo_populate_postgres()\n",
    "# 12 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.path.join(os.getcwd(), os.pardir), os.pardir)\n",
    "with open(os.path.join(data_dir, 'all.pkl'), 'rb') as fp:\n",
    "    df = pickle.load(fp)\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sqlalchemy.create_engine(\"postgresql+psycopg://{user}:{pw}@localhost/{db}\"\n",
    "                       .format(user=\"postgres\",\n",
    "                               pw=\"shimul\",\n",
    "                               db=\"postgres\"))\n",
    "with engine.begin() as connection:\n",
    "    df.to_sql(con=connection,\n",
    "            name='versioninfo',\n",
    "            if_exists='append',\n",
    "            index=False,\n",
    "            dtype={\n",
    "                        'system_name': sqlalchemy.types.VARCHAR,\n",
    "                        'package_name': sqlalchemy.types.VARCHAR,\n",
    "                        'version_name': sqlalchemy.types.VARCHAR,\n",
    "                        'release_date': sqlalchemy.types.TIMESTAMP,\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## relations populate in POSTGRES (obsolete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relations_populate_postgres():\n",
    "    engine = sqlalchemy.create_engine(\"postgresql+psycopg://{user}:{pw}@localhost/{db}\"\n",
    "                       .format(user=\"postgres\",\n",
    "                               pw=\"shimul\",\n",
    "                               db=\"postgres\"))\n",
    "    data_dir = os.path.join(os.getcwd(), \"requirement-durations\")\n",
    "    total_number_of_rows = 0\n",
    "    for csv_file in os.listdir(data_dir):\n",
    "        # System;Name;Version;RequirementName;RequirementVersion;HighestMatchingVersion;HighestAvailableRelease;IntervalStart;IntervalEnd;IsOutOfDate;IsRegular;Warnings\n",
    "        df = pd.read_csv(os.path.join(data_dir, csv_file),\n",
    "                        header=0,\n",
    "                        sep=';',\n",
    "                        on_bad_lines='skip',\n",
    "                        usecols=['System', 'Name', 'Version', 'RequirementName', 'RequirementVersion', 'HighestMatchingVersion', 'HighestAvailableRelease', 'IntervalStart', 'IntervalEnd', 'IsOutOfDate', 'IsRegular', 'Warnings'],\n",
    "                        )\n",
    "        df.columns = ['system_name', 'from_package_name', 'from_version', 'to_package_name', 'actual_requirement', 'to_version', 'to_package_highest_available_release', 'interval_start', 'interval_end', 'is_out_of_date', 'is_regular', 'warnings']\n",
    "        # We don't want to drop all rows having NULL values.\n",
    "        # df = df.dropna()\n",
    "        df.dropna(subset=['system_name', 'from_package_name', 'from_version', 'to_package_name', 'to_version'], inplace=True)\n",
    "        df = df.drop_duplicates()\n",
    "        with engine.begin() as connection:\n",
    "            df.to_sql(con=connection,\n",
    "                      name='relations',\n",
    "                      if_exists='append',\n",
    "                      index=False,\n",
    "                      dtype={\n",
    "                                'system_name': sqlalchemy.types.VARCHAR,\n",
    "                                'from_package_name': sqlalchemy.types.VARCHAR,\n",
    "                                'from_version': sqlalchemy.types.VARCHAR,\n",
    "                                'to_package_name': sqlalchemy.types.VARCHAR,\n",
    "                                'actual_requirement': sqlalchemy.types.VARCHAR,\n",
    "                                'to_version': sqlalchemy.types.VARCHAR,\n",
    "                                'to_package_highest_available_release': sqlalchemy.types.VARCHAR,\n",
    "                                'interval_start': sqlalchemy.types.TIMESTAMP,\n",
    "                                'interval_end': sqlalchemy.types.TIMESTAMP,\n",
    "                                'is_out_of_date': sqlalchemy.types.BOOLEAN,\n",
    "                                'is_regular': sqlalchemy.types.BOOLEAN,\n",
    "                                'warnings': sqlalchemy.types.VARCHAR\n",
    "                            })\n",
    "        rows = df.shape[0]\n",
    "        total_number_of_rows += rows\n",
    "        print(f\"wrote data from file: {csv_file}, rows: {rows}, total_rows: {total_number_of_rows}\")\n",
    "\n",
    "relations_populate_postgres()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is an error like psycopg does not have any attribute 'misc', run the import cell again (mainly psycopg-binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relations populate in POSTGRES from avro data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avro_df(filepath, mode):\n",
    "    columns = ['System', 'Name', 'Version', 'RequirementName', 'RequirementVersion', 'HighestMatchingVersion', 'HighestAvailableRelease', 'IntervalStart', 'IntervalEnd', 'IsOutOfDate', 'IsRegular', 'Warnings']\n",
    "    # Open file stream\n",
    "    with open(filepath, mode) as fp:\n",
    "        # Configure Avro reader\n",
    "        reader = fastavro.reader(fp)\n",
    "        # Load records in memory\n",
    "        # records = [r for r in reader]\n",
    "        # Populate pandas.DataFrame with records\n",
    "        df = pd.DataFrame.from_records([r for r in reader], columns=columns)\n",
    "        # Return created DataFrame\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_avro(filepath):\n",
    "    # Read data from an avro file\n",
    "    with open(filepath, 'rb') as f:\n",
    "        reader = DataFileReader(f, DatumReader())\n",
    "        metadata = copy.deepcopy(reader.meta)\n",
    "        schema_from_file = json.loads(metadata['avro.schema'])\n",
    "        records = [record for record in reader]\n",
    "        reader.close()\n",
    "    return schema_from_file, records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psql_insert_copy(table, conn, keys, data_iter):\n",
    "    # gets a DBAPI connection that can provide a cursor\n",
    "    dbapi_conn = conn.connection\n",
    "    with dbapi_conn.cursor() as cur:\n",
    "        s_buf = StringIO()\n",
    "        writer = csv.writer(s_buf)\n",
    "        writer.writerows(data_iter)\n",
    "        s_buf.seek(0)\n",
    "\n",
    "        columns = ', '.join('\"{}\"'.format(k) for k in keys)\n",
    "        if table.schema:\n",
    "            table_name = '{}.{}'.format(table.schema, table.name)\n",
    "        else:\n",
    "            table_name = table.name\n",
    "\n",
    "        sql = 'COPY {} ({}) FROM STDIN WITH CSV'.format(\n",
    "            table_name, columns)\n",
    "        cur.copy_expert(sql=sql, file=s_buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_this_file_has_been_processed(file_to_check):\n",
    "    with open('done.txt', 'r') as file, open('error.txt', 'r') as error_file:\n",
    "        lines = file.readlines()\n",
    "        lines = [line.strip() for line in lines]\n",
    "        errors = error_file.readlines()\n",
    "        errors = [error.strip() for error in errors]\n",
    "        # print (lines)\n",
    "        # print (file)\n",
    "        if file_to_check in lines or file_to_check in errors:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "def mark_this_file_as_processed(file_to_write):\n",
    "    with open('done.txt', 'a') as file:\n",
    "        file.write(file_to_write + '\\n')\n",
    "\n",
    "def mark_this_file_as_error(file_to_write):\n",
    "    with open('error.txt', 'a') as file:\n",
    "        file.write(file_to_write + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "check_if_this_file_has_been_processed('000000176346')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sqlalchemy.engine.base.Connection object at 0x79e1e16eabd0>\n",
      "file 000000176346 has already been processed\n",
      "file 000000176331 has already been processed\n",
      "file 000000175727 has already been processed\n",
      "file 000000175750 has already been processed\n",
      "file 000000175657 has already been processed\n",
      "file 000000175620 has already been processed\n",
      "file 000000176236 has already been processed\n",
      "file 000000176241 has already been processed\n",
      "file 000000175979 has already been processed\n",
      "file 000000175874 has already been processed\n",
      "file 000000175803 has already been processed\n",
      "file 000000175997 has already been processed\n",
      "file 000000176408 has already been processed\n",
      "file 000000176142 has already been processed\n",
      "file 000000176135 has already been processed\n",
      "file 000000176038 has already been processed\n",
      "file 000000176148 has already been processed\n",
      "file 000000176032 has already been processed\n",
      "file 000000176045 has already been processed\n",
      "file 000000175890 has already been processed\n",
      "file 000000175904 has already been processed\n",
      "file 000000175973 has already been processed\n",
      "file 000000175809 has already been processed\n",
      "file 000000176402 has already been processed\n",
      "file 000000175618 has already been processed\n",
      "file 000000175762 has already been processed\n",
      "file 000000175715 has already been processed\n",
      "file 000000175681 has already been processed\n",
      "file 000000176279 has already been processed\n",
      "file 000000176303 has already been processed\n",
      "file 000000176297 has already been processed\n",
      "file 000000176374 has already been processed\n",
      "file 000000176273 has already been processed\n",
      "file 000000176390 has already been processed\n",
      "file 000000176204 has already been processed\n",
      "file 000000176309 has already been processed\n",
      "file 000000175786 has already been processed\n",
      "file 000000175612 has already been processed\n",
      "file 000000175665 has already been processed\n",
      "file 000000175768 has already been processed\n",
      "file 000000176107 has already been processed\n",
      "file 000000176093 has already been processed\n",
      "file 000000176170 has already been processed\n",
      "file 000000175831 has already been processed\n",
      "file 000000175846 has already been processed\n",
      "file 000000175588 has already been processed\n",
      "file 000000175941 has already been processed\n",
      "file 000000175936 has already been processed\n",
      "file 000000175582 has already been processed\n",
      "file 000000176099 has already been processed\n",
      "file 000000176077 has already been processed\n",
      "file 000000176194 has already been processed\n",
      "file 000000176000 has already been processed\n",
      "file 000000176328 has already been processed\n",
      "file 000000176225 has already been processed\n",
      "file 000000176252 has already been processed\n",
      "file 000000175749 has already been processed\n",
      "file 000000175644 has already been processed\n",
      "file 000000175633 has already been processed\n",
      "file 000000175734 has already been processed\n",
      "file 000000175743 has already been processed\n",
      "file 000000175639 has already been processed\n",
      "file 000000176355 has already been processed\n",
      "file 000000176322 has already been processed\n",
      "file 000000176258 has already been processed\n",
      "file 000000175883 has already been processed\n",
      "file 000000175917 has already been processed\n",
      "file 000000175960 has already been processed\n",
      "file 000000176411 has already been processed\n",
      "file 000000176021 has already been processed\n",
      "file 000000176056 has already been processed\n",
      "file 000000176151 has already been processed\n",
      "file 000000176126 has already been processed\n",
      "file 000000175867 has already been processed\n",
      "file 000000175810 has already been processed\n",
      "file 000000175984 has already been processed\n",
      "file 000000175889 has already been processed\n",
      "file 000000175698 has already been processed\n",
      "file 000000175795 has already been processed\n",
      "file 000000175601 has already been processed\n",
      "file 000000175676 has already been processed\n",
      "file 000000176260 has already been processed\n",
      "file 000000176383 has already been processed\n",
      "file 000000176217 has already been processed\n",
      "file 000000176310 has already been processed\n",
      "file 000000176284 has already been processed\n",
      "file 000000176367 has already been processed\n",
      "file 000000176389 has already been processed\n",
      "file 000000175771 has already been processed\n",
      "file 000000175706 has already been processed\n",
      "file 000000175692 has already been processed\n",
      "file 000000176064 has already been processed\n",
      "file 000000176187 has already been processed\n",
      "file 000000176013 has already been processed\n",
      "file 000000176169 has already been processed\n",
      "file 000000175591 has already been processed\n",
      "file 000000175828 has already been processed\n",
      "file 000000175952 has already been processed\n",
      "file 000000175925 has already been processed\n",
      "file 000000175822 has already been processed\n",
      "file 000000175855 has already been processed\n",
      "file 000000175575 has already been processed\n",
      "file 000000175958 has already been processed\n",
      "file 000000176019 has already been processed\n",
      "file 000000176114 has already been processed\n",
      "file 000000176080 has already been processed\n",
      "file 000000176163 has already been processed\n",
      "file 000000176119 has already been processed\n",
      "file 000000176014 has already been processed\n",
      "file 000000176180 has already been processed\n",
      "file 000000176063 has already been processed\n",
      "file 000000175578 has already been processed\n",
      "file 000000175922 has already been processed\n",
      "file 000000175955 has already been processed\n",
      "file 000000175596 has already been processed\n",
      "file 000000175858 has already been processed\n",
      "file 000000175928 has already been processed\n",
      "file 000000175852 has already been processed\n",
      "file 000000175825 has already been processed\n",
      "file 000000176164 has already been processed\n",
      "file 000000176087 has already been processed\n",
      "file 000000176113 has already been processed\n",
      "file 000000176069 has already been processed\n",
      "file 000000175671 has already been processed\n",
      "file 000000175606 has already been processed\n",
      "file 000000175792 has already been processed\n",
      "file 000000176210 has already been processed\n",
      "file 000000176384 has already been processed\n",
      "file 000000176267 has already been processed\n",
      "file 000000176289 has already been processed\n",
      "file 000000176360 has already been processed\n",
      "file 000000176283 has already been processed\n",
      "file 000000176317 has already been processed\n",
      "file 000000175798 has already been processed\n",
      "file 000000175695 has already been processed\n",
      "file 000000175701 has already been processed\n",
      "file 000000175776 has already been processed\n",
      "file 000000175967 has already been processed\n",
      "file 000000175910 has already been processed\n",
      "file 000000175884 has already been processed\n",
      "file 000000175989 has already been processed\n",
      "file 000000176416 has already been processed\n",
      "file 000000176051 has already been processed\n",
      "file 000000176026 has already been processed\n",
      "file 000000176121 has already been processed\n",
      "file 000000176156 has already been processed\n",
      "file 000000175983 has already been processed\n",
      "file 000000175817 has already been processed\n",
      "file 000000175860 has already been processed\n",
      "file 000000176255 has already been processed\n",
      "file 000000176222 has already been processed\n",
      "file 000000176358 has already been processed\n",
      "file 000000175634 has already been processed\n",
      "file 000000175643 has already been processed\n",
      "file 000000175739 has already been processed\n",
      "file 000000175649 has already been processed\n",
      "file 000000175744 has already been processed\n",
      "file 000000175733 has already been processed\n",
      "file 000000176228 has already been processed\n",
      "file 000000176325 has already been processed\n",
      "file 000000176352 has already been processed\n",
      "file 000000176199 has already been processed\n",
      "file 000000176177 has already been processed\n",
      "file 000000176094 has already been processed\n",
      "file 000000176100 has already been processed\n",
      "file 000000175841 has already been processed\n",
      "file 000000175836 has already been processed\n",
      "file 000000175585 has already been processed\n",
      "file 000000175931 has already been processed\n",
      "file 000000175946 has already been processed\n",
      "file 000000176007 has already been processed\n",
      "file 000000176193 has already been processed\n",
      "file 000000176070 has already been processed\n",
      "file 000000175686 has already been processed\n",
      "file 000000175712 has already been processed\n",
      "file 000000175765 has already been processed\n",
      "file 000000175668 has already been processed\n",
      "file 000000176373 has already been processed\n",
      "file 000000176290 has already been processed\n",
      "file 000000176304 has already been processed\n",
      "file 000000176209 has already been processed\n",
      "file 000000176379 has already been processed\n",
      "file 000000176203 has already been processed\n",
      "file 000000176397 has already been processed\n",
      "file 000000176274 has already been processed\n",
      "file 000000175718 has already been processed\n",
      "file 000000175662 has already been processed\n",
      "file 000000175615 has already been processed\n",
      "file 000000175781 has already been processed\n",
      "file 000000175990 has already been processed\n",
      "file 000000175804 has already been processed\n",
      "file 000000175873 has already been processed\n",
      "file 000000175909 has already been processed\n",
      "file 000000176048 has already been processed\n",
      "file 000000176132 has already been processed\n",
      "file 000000176145 has already been processed\n",
      "file 000000176042 has already been processed\n",
      "file 000000176035 has already been processed\n",
      "file 000000176138 has already been processed\n",
      "file 000000175879 has already been processed\n",
      "file 000000175974 has already been processed\n",
      "file 000000175903 has already been processed\n",
      "file 000000175897 has already been processed\n",
      "file 000000176405 has already been processed\n",
      "file 000000176336 has already been processed\n",
      "file 000000176341 has already been processed\n",
      "file 000000175757 has already been processed\n",
      "file 000000175720 has already been processed\n",
      "file 000000175627 has already been processed\n",
      "file 000000175650 has already been processed\n",
      "file 000000176246 has already been processed\n",
      "file 000000176231 has already been processed\n",
      "file 000000176401 has already been processed\n",
      "file 000000175970 has already been processed\n",
      "file 000000175893 has already been processed\n",
      "file 000000175907 has already been processed\n",
      "file 000000176046 has already been processed\n",
      "file 000000176031 has already been processed\n",
      "file 000000176136 has already been processed\n",
      "file 000000176141 has already been processed\n",
      "file 000000175899 has already been processed\n",
      "file 000000175800 has already been processed\n",
      "file 000000175994 has already been processed\n",
      "file 000000175877 has already been processed\n",
      "file 000000176242 has already been processed\n",
      "file 000000176235 has already been processed\n",
      "file 000000176338 has already been processed\n",
      "file 000000175623 has already been processed\n",
      "file 000000175654 has already been processed\n",
      "file 000000175759 has already been processed\n",
      "file 000000175629 has already been processed\n",
      "file 000000175753 has already been processed\n",
      "file 000000175724 has already been processed\n",
      "file 000000176248 has already been processed\n",
      "file 000000176332 has already been processed\n",
      "file 000000176345 has already been processed\n",
      "file 000000176179 has already been processed\n",
      "file 000000176197 has already been processed\n",
      "file 000000176003 has already been processed\n",
      "file 000000176074 has already been processed\n",
      "file 000000175935 has already been processed\n",
      "file 000000175942 has already been processed\n",
      "file 000000175838 has already been processed\n",
      "file 000000175581 has already been processed\n",
      "file 000000175948 has already been processed\n",
      "file 000000175845 has already been processed\n",
      "file 000000175832 has already been processed\n",
      "file 000000176173 has already been processed\n",
      "file 000000176104 has already been processed\n",
      "file 000000176090 has already been processed\n",
      "file 000000176009 has already been processed\n",
      "file 000000175666 has already been processed\n",
      "file 000000175785 has already been processed\n",
      "file 000000175611 has already been processed\n",
      "file 000000175688 has already been processed\n",
      "file 000000176393 has already been processed\n",
      "file 000000176207 has already been processed\n",
      "file 000000176270 has already been processed\n",
      "file 000000176399 has already been processed\n",
      "file 000000176377 has already been processed\n",
      "file 000000176300 has already been processed\n",
      "file 000000176294 has already been processed\n",
      "file 000000175716 has already been processed\n",
      "file 000000175682 has already been processed\n",
      "file 000000175761 has already been processed\n",
      "file 000000176418 has already been processed\n",
      "file 000000175813 has already been processed\n",
      "file 000000175987 has already been processed\n",
      "file 000000175864 has already been processed\n",
      "file 000000175969 has already been processed\n",
      "file 000000176028 has already been processed\n",
      "file 000000176125 has already been processed\n",
      "file 000000176152 has already been processed\n",
      "file 000000176055 has already been processed\n",
      "file 000000176022 has already been processed\n",
      "file 000000176158 has already been processed\n",
      "file 000000176412 has already been processed\n",
      "file 000000175819 has already been processed\n",
      "file 000000175963 has already been processed\n",
      "file 000000175880 has already been processed\n",
      "file 000000175914 has already been processed\n",
      "file 000000176321 has already been processed\n",
      "file 000000176356 has already been processed\n",
      "file 000000175740 has already been processed\n",
      "file 000000175737 has already been processed\n",
      "file 000000175630 has already been processed\n",
      "file 000000175647 has already been processed\n",
      "file 000000176251 has already been processed\n",
      "file 000000176226 has already been processed\n",
      "file 000000176160 has already been processed\n",
      "file 000000176117 has already been processed\n",
      "file 000000176083 has already been processed\n",
      "file 000000175598 has already been processed\n",
      "file 000000175856 has already been processed\n",
      "file 000000175821 has already been processed\n",
      "file 000000175576 has already been processed\n",
      "file 000000175592 has already been processed\n",
      "file 000000175926 has already been processed\n",
      "file 000000175951 has already been processed\n",
      "file 000000176184 has already been processed\n",
      "file 000000176010 has already been processed\n",
      "file 000000176067 has already been processed\n",
      "file 000000176089 has already been processed\n",
      "file 000000175705 has already been processed\n",
      "file 000000175691 has already been processed\n",
      "file 000000175772 has already been processed\n",
      "file 000000175608 has already been processed\n",
      "file 000000176364 has already been processed\n",
      "file 000000176313 has already been processed\n",
      "file 000000176287 has already been processed\n",
      "file 000000176269 has already been processed\n",
      "file 000000176319 has already been processed\n",
      "file 000000176380 has already been processed\n",
      "file 000000176214 has already been processed\n",
      "file 000000176263 has already been processed\n",
      "file 000000175778 has already been processed\n",
      "file 000000175675 has already been processed\n",
      "file 000000175796 has already been processed\n",
      "file 000000175602 has already been processed\n",
      "file 000000175678 has already been processed\n",
      "file 000000175775 has already been processed\n",
      "file 000000175696 has already been processed\n",
      "file 000000175702 has already been processed\n",
      "file 000000176219 has already been processed\n",
      "file 000000176280 has already been processed\n",
      "file 000000176314 has already been processed\n",
      "file 000000176363 has already been processed\n",
      "file 000000176264 has already been processed\n",
      "file 000000176213 has already been processed\n",
      "file 000000176387 has already been processed\n",
      "file 000000176369 has already been processed\n",
      "file 000000175605 has already been processed\n",
      "file 000000175791 has already been processed\n",
      "file 000000175672 has already been processed\n",
      "file 000000175708 has already been processed\n",
      "file 000000176084 has already been processed\n",
      "file 000000176110 has already been processed\n",
      "file 000000176167 has already been processed\n",
      "file 000000176189 has already been processed\n",
      "file 000000175826 has already been processed\n",
      "file 000000175851 has already been processed\n",
      "file 000000175956 has already been processed\n",
      "file 000000175921 has already been processed\n",
      "file 000000175595 has already been processed\n",
      "file 000000176060 has already been processed\n",
      "file 000000176017 has already been processed\n",
      "file 000000176183 has already been processed\n",
      "file 000000176351 has already been processed\n",
      "file 000000176326 has already been processed\n",
      "file 000000175730 has already been processed\n",
      "file 000000175747 has already been processed\n",
      "file 000000175640 has already been processed\n",
      "file 000000175637 has already been processed\n",
      "file 000000176221 has already been processed\n",
      "file 000000176256 has already been processed\n",
      "file 000000175919 has already been processed\n",
      "file 000000175863 has already been processed\n",
      "file 000000175980 has already been processed\n",
      "file 000000175814 has already been processed\n",
      "file 000000176155 has already been processed\n",
      "file 000000176122 has already been processed\n",
      "file 000000176058 has already been processed\n",
      "file 000000176128 has already been processed\n",
      "file 000000176025 has already been processed\n",
      "file 000000176052 has already been processed\n",
      "file 000000176415 has already been processed\n",
      "file 000000175913 has already been processed\n",
      "file 000000175887 has already been processed\n",
      "file 000000175964 has already been processed\n",
      "file 000000175869 has already been processed\n",
      "file 000000175616 has already been processed\n",
      "file 000000175782 has already been processed\n",
      "file 000000175661 has already been processed\n",
      "file 000000176299 has already been processed\n",
      "file 000000176277 has already been processed\n",
      "file 000000176200 has already been processed\n",
      "file 000000176394 has already been processed\n",
      "file 000000176293 has already been processed\n",
      "file 000000176307 has already been processed\n",
      "file 000000176370 has already been processed\n",
      "file 000000175766 has already been processed\n",
      "file 000000175685 has already been processed\n",
      "file 000000175711 has already been processed\n",
      "file 000000175788 has already been processed\n",
      "file 000000176073 has already been processed\n",
      "file 000000176004 has already been processed\n",
      "file 000000176190 has already been processed\n",
      "file 000000176109 has already been processed\n",
      "file 000000175848 has already been processed\n",
      "file 000000175586 has already been processed\n",
      "file 000000175945 has already been processed\n",
      "file 000000175932 has already been processed\n",
      "file 000000175835 has already been processed\n",
      "file 000000175842 has already been processed\n",
      "file 000000175938 has already been processed\n",
      "file 000000176079 has already been processed\n",
      "file 000000176097 has already been processed\n",
      "file 000000176103 has already been processed\n",
      "file 000000176174 has already been processed\n",
      "file 000000176348 has already been processed\n",
      "file 000000176232 has already been processed\n",
      "file 000000176245 has already been processed\n",
      "file 000000175729 has already been processed\n",
      "file 000000175653 has already been processed\n",
      "file 000000175624 has already been processed\n",
      "file 000000175723 has already been processed\n",
      "file 000000175754 has already been processed\n",
      "file 000000175659 has already been processed\n",
      "file 000000176342 has already been processed\n",
      "file 000000176335 has already been processed\n",
      "file 000000176238 has already been processed\n",
      "file 000000176406 has already been processed\n",
      "file 000000175999 has already been processed\n",
      "file 000000175900 has already been processed\n",
      "file 000000175894 has already been processed\n",
      "file 000000175977 has already been processed\n",
      "file 000000176036 has already been processed\n",
      "file 000000176041 has already been processed\n",
      "file 000000176146 has already been processed\n",
      "file 000000176131 has already been processed\n",
      "file 000000175870 has already been processed\n",
      "file 000000175993 has already been processed\n",
      "file 000000175807 has already been processed\n",
      "file 000000176071 has already been processed\n",
      "file 000000176006 has already been processed\n",
      "file 000000176192 has already been processed\n",
      "file 000000175584 has already been processed\n",
      "file 000000175947 has already been processed\n",
      "file 000000175930 has already been processed\n",
      "file 000000175837 has already been processed\n",
      "file 000000175840 has already been processed\n",
      "file 000000176198 has already been processed\n",
      "file 000000176095 has already been processed\n",
      "file 000000176101 has already been processed\n",
      "file 000000176176 has already been processed\n",
      "file 000000175719 has already been processed\n",
      "file 000000175614 has already been processed\n",
      "file 000000175780 has already been processed\n",
      "file 000000175663 has already been processed\n",
      "file 000000176378 has already been processed\n",
      "file 000000176275 has already been processed\n",
      "file 000000176202 has already been processed\n",
      "file 000000176396 has already been processed\n",
      "file 000000176291 has already been processed\n",
      "file 000000176305 has already been processed\n",
      "file 000000176372 has already been processed\n",
      "file 000000176208 has already been processed\n",
      "file 000000175764 has already been processed\n",
      "file 000000175687 has already been processed\n",
      "file 000000175713 has already been processed\n",
      "file 000000175669 has already been processed\n",
      "file 000000175878 has already been processed\n",
      "file 000000175902 has already been processed\n",
      "file 000000175896 has already been processed\n",
      "file 000000175975 has already been processed\n",
      "file 000000176404 has already been processed\n",
      "file 000000176034 has already been processed\n",
      "file 000000176043 has already been processed\n",
      "file 000000176139 has already been processed\n",
      "file 000000176049 has already been processed\n",
      "file 000000176144 has already been processed\n",
      "file 000000176133 has already been processed\n",
      "file 000000175872 has already been processed\n",
      "file 000000175991 has already been processed\n",
      "file 000000175805 has already been processed\n",
      "file 000000175908 has already been processed\n",
      "file 000000176230 has already been processed\n",
      "file 000000176247 has already been processed\n",
      "file 000000175651 has already been processed\n",
      "file 000000175626 has already been processed\n",
      "file 000000175721 has already been processed\n",
      "file 000000175756 has already been processed\n",
      "file 000000176340 has already been processed\n",
      "file 000000176337 has already been processed\n",
      "file 000000176086 has already been processed\n",
      "file 000000176112 has already been processed\n",
      "file 000000176165 has already been processed\n",
      "file 000000176068 has already been processed\n",
      "file 000000175929 has already been processed\n",
      "file 000000175824 has already been processed\n",
      "file 000000175853 has already been processed\n",
      "file 000000175954 has already been processed\n",
      "file 000000175579 has already been processed\n",
      "file 000000175923 has already been processed\n",
      "file 000000175859 has already been processed\n",
      "file 000000175597 has already been processed\n",
      "file 000000176118 has already been processed\n",
      "file 000000176062 has already been processed\n",
      "file 000000176015 has already been processed\n",
      "file 000000176181 has already been processed\n",
      "file 000000175799 has already been processed\n",
      "file 000000175777 has already been processed\n",
      "file 000000175694 has already been processed\n",
      "file 000000175700 has already been processed\n",
      "file 000000176282 has already been processed\n",
      "file 000000176316 has already been processed\n",
      "file 000000176361 has already been processed\n",
      "file 000000176266 has already been processed\n",
      "file 000000176211 has already been processed\n",
      "file 000000176385 has already been processed\n",
      "file 000000176288 has already been processed\n",
      "file 000000175607 has already been processed\n",
      "file 000000175793 has already been processed\n",
      "file 000000175670 has already been processed\n",
      "file 000000175861 has already been processed\n",
      "file 000000175982 has already been processed\n",
      "file 000000175816 has already been processed\n",
      "file 000000176157 has already been processed\n",
      "file 000000176120 has already been processed\n",
      "file 000000176027 has already been processed\n",
      "file 000000176050 has already been processed\n",
      "file 000000175911 has already been processed\n",
      "file 000000175885 has already been processed\n",
      "file 000000175966 has already been processed\n",
      "file 000000175988 has already been processed\n",
      "file 000000176417 has already been processed\n",
      "file 000000176229 has already been processed\n",
      "file 000000176353 has already been processed\n",
      "file 000000176324 has already been processed\n",
      "file 000000175648 has already been processed\n",
      "file 000000175732 has already been processed\n",
      "file 000000175745 has already been processed\n",
      "file 000000175642 has already been processed\n",
      "file 000000175635 has already been processed\n",
      "file 000000175738 has already been processed\n",
      "file 000000176223 has already been processed\n",
      "file 000000176254 has already been processed\n",
      "file 000000176359 has already been processed\n",
      "file 000000176323 has already been processed\n",
      "file 000000176354 has already been processed\n",
      "file 000000176259 has already been processed\n",
      "file 000000175742 has already been processed\n",
      "file 000000175735 has already been processed\n",
      "file 000000175638 has already been processed\n",
      "file 000000175748 has already been processed\n",
      "file 000000175632 has already been processed\n",
      "file 000000175645 has already been processed\n",
      "file 000000176329 has already been processed\n",
      "file 000000176253 has already been processed\n",
      "file 000000176224 has already been processed\n",
      "file 000000175811 has already been processed\n",
      "file 000000175985 has already been processed\n",
      "file 000000175866 has already been processed\n",
      "file 000000175888 has already been processed\n",
      "file 000000176127 has already been processed\n",
      "file 000000176150 has already been processed\n",
      "file 000000176057 has already been processed\n",
      "file 000000176020 has already been processed\n",
      "file 000000175961 has already been processed\n",
      "file 000000175882 has already been processed\n",
      "file 000000175916 has already been processed\n",
      "file 000000176410 has already been processed\n",
      "file 000000175707 has already been processed\n",
      "file 000000175693 has already been processed\n",
      "file 000000175770 has already been processed\n",
      "file 000000176366 has already been processed\n",
      "file 000000176311 has already been processed\n",
      "file 000000176285 has already been processed\n",
      "file 000000176388 has already been processed\n",
      "file 000000176382 has already been processed\n",
      "file 000000176216 has already been processed\n",
      "file 000000176261 has already been processed\n",
      "file 000000175699 has already been processed\n",
      "file 000000175677 has already been processed\n",
      "file 000000175794 has already been processed\n",
      "file 000000175600 has already been processed\n",
      "file 000000176018 has already been processed\n",
      "file 000000176162 has already been processed\n",
      "file 000000176115 has already been processed\n",
      "file 000000176081 has already been processed\n",
      "file 000000175854 has already been processed\n",
      "file 000000175823 has already been processed\n",
      "file 000000175959 has already been processed\n",
      "file 000000175574 has already been processed\n",
      "file 000000175829 has already been processed\n",
      "file 000000175590 has already been processed\n",
      "file 000000175924 has already been processed\n",
      "file 000000175953 has already been processed\n",
      "file 000000176186 has already been processed\n",
      "file 000000176012 has already been processed\n",
      "file 000000176065 has already been processed\n",
      "file 000000176168 has already been processed\n",
      "file 000000176240 has already been processed\n",
      "file 000000176237 has already been processed\n",
      "file 000000175621 has already been processed\n",
      "file 000000175656 has already been processed\n",
      "file 000000175751 has already been processed\n",
      "file 000000175726 has already been processed\n",
      "file 000000176330 has already been processed\n",
      "file 000000176347 has already been processed\n",
      "file 000000175972 has already been processed\n",
      "file 000000175891 has already been processed\n",
      "file 000000175905 has already been processed\n",
      "file 000000175808 has already been processed\n",
      "file 000000176403 has already been processed\n",
      "file 000000176149 has already been processed\n",
      "file 000000176044 has already been processed\n",
      "file 000000176033 has already been processed\n",
      "file 000000176134 has already been processed\n",
      "file 000000176143 has already been processed\n",
      "file 000000176039 has already been processed\n",
      "file 000000175978 has already been processed\n",
      "file 000000175802 has already been processed\n",
      "file 000000175996 has already been processed\n",
      "file 000000175875 has already been processed\n",
      "file 000000176409 has already been processed\n",
      "file 000000175664 has already been processed\n",
      "file 000000175787 has already been processed\n",
      "file 000000175613 has already been processed\n",
      "file 000000175769 has already been processed\n",
      "file 000000176391 has already been processed\n",
      "file 000000176205 has already been processed\n",
      "file 000000176272 has already been processed\n",
      "file 000000176308 has already been processed\n",
      "file 000000176278 has already been processed\n",
      "file 000000176375 has already been processed\n",
      "file 000000176302 has already been processed\n",
      "file 000000176296 has already been processed\n",
      "file 000000175619 has already been processed\n",
      "file 000000175714 has already been processed\n",
      "file 000000175680 has already been processed\n",
      "file 000000175763 has already been processed\n",
      "file 000000176098 has already been processed\n",
      "file 000000176195 has already been processed\n",
      "file 000000176001 has already been processed\n",
      "file 000000176076 has already been processed\n",
      "file 000000175937 has already been processed\n",
      "file 000000175940 has already been processed\n",
      "file 000000175583 has already been processed\n",
      "file 000000175589 has already been processed\n",
      "file 000000175847 has already been processed\n",
      "file 000000175830 has already been processed\n",
      "file 000000176171 has already been processed\n",
      "file 000000176106 has already been processed\n",
      "file 000000176092 has already been processed\n",
      "file 000000175684 has already been processed\n",
      "file 000000175710 has already been processed\n",
      "file 000000175767 has already been processed\n",
      "file 000000175789 has already been processed\n",
      "file 000000176371 has already been processed\n",
      "file 000000176292 has already been processed\n",
      "file 000000176306 has already been processed\n",
      "file 000000176298 has already been processed\n",
      "file 000000176201 has already been processed\n",
      "file 000000176395 has already been processed\n",
      "file 000000176276 has already been processed\n",
      "file 000000175660 has already been processed\n",
      "file 000000175617 has already been processed\n",
      "file 000000175783 has already been processed\n",
      "file 000000176078 has already been processed\n",
      "file 000000176175 has already been processed\n",
      "file 000000176096 has already been processed\n",
      "file 000000176102 has already been processed\n",
      "file 000000175843 has already been processed\n",
      "file 000000175834 has already been processed\n",
      "file 000000175939 has already been processed\n",
      "file 000000175587 has already been processed\n",
      "file 000000175849 has already been processed\n",
      "file 000000175933 has already been processed\n",
      "file 000000175944 has already been processed\n",
      "file 000000176005 has already been processed\n",
      "file 000000176191 has already been processed\n",
      "file 000000176072 has already been processed\n",
      "file 000000176108 has already been processed\n",
      "file 000000176334 has already been processed\n",
      "file 000000176343 has already been processed\n",
      "file 000000176239 has already been processed\n",
      "file 000000175755 has already been processed\n",
      "file 000000175722 has already been processed\n",
      "file 000000175658 has already been processed\n",
      "file 000000175728 has already been processed\n",
      "file 000000175625 has already been processed\n",
      "file 000000175652 has already been processed\n",
      "file 000000176349 has already been processed\n",
      "file 000000176244 has already been processed\n",
      "file 000000176233 has already been processed\n",
      "file 000000175992 has already been processed\n",
      "file 000000175806 has already been processed\n",
      "file 000000175871 has already been processed\n",
      "started processing file: 000000176130\n",
      "wrote data from 1 file: 000000176130, rows: 173727, total_rows: 173727\n",
      "started processing file: 000000176147\n",
      "wrote data from 2 file: 000000176147, rows: 235139, total_rows: 408866\n",
      "started processing file: 000000176040\n",
      "wrote data from 3 file: 000000176040, rows: 4482721, total_rows: 4891587\n",
      "started processing file: 000000176037\n",
      "wrote data from 4 file: 000000176037, rows: 2994284, total_rows: 7885871\n",
      "started processing file: 000000176407\n",
      "wrote data from 5 file: 000000176407, rows: 136520, total_rows: 8022391\n",
      "started processing file: 000000175998\n",
      "wrote data from 6 file: 000000175998, rows: 3949292, total_rows: 11971683\n",
      "started processing file: 000000175976\n",
      "wrote data from 7 file: 000000175976, rows: 3261406, total_rows: 15233089\n",
      "started processing file: 000000175901\n",
      "wrote data from 8 file: 000000175901, rows: 3746723, total_rows: 18979812\n",
      "started processing file: 000000175895\n",
      "wrote data from 9 file: 000000175895, rows: 2578304, total_rows: 21558116\n",
      "started processing file: 000000175673\n",
      "wrote data from 10 file: 000000175673, rows: 789111, total_rows: 22347227\n",
      "started processing file: 000000175604\n",
      "wrote data from 11 file: 000000175604, rows: 3861639, total_rows: 26208866\n",
      "started processing file: 000000175790\n",
      "wrote data from 12 file: 000000175790, rows: 3089307, total_rows: 29298173\n",
      "started processing file: 000000175709\n",
      "wrote data from 13 file: 000000175709, rows: 3564299, total_rows: 32862472\n",
      "started processing file: 000000176212\n",
      "wrote data from 14 file: 000000176212, rows: 195998, total_rows: 33058470\n",
      "started processing file: 000000176386\n",
      "wrote data from 15 file: 000000176386, rows: 124196, total_rows: 33182666\n",
      "started processing file: 000000176265\n",
      "wrote data from 16 file: 000000176265, rows: 116936, total_rows: 33299602\n",
      "started processing file: 000000176368\n",
      "wrote data from 17 file: 000000176368, rows: 138634, total_rows: 33438236\n",
      "started processing file: 000000176218\n",
      "wrote data from 18 file: 000000176218, rows: 188680, total_rows: 33626916\n",
      "started processing file: 000000176362\n",
      "wrote data from 19 file: 000000176362, rows: 245386, total_rows: 33872302\n",
      "started processing file: 000000176281\n",
      "wrote data from 20 file: 000000176281, rows: 120756, total_rows: 33993058\n",
      "started processing file: 000000176315\n",
      "wrote data from 21 file: 000000176315, rows: 123341, total_rows: 34116399\n",
      "started processing file: 000000175679\n",
      "wrote data from 22 file: 000000175679, rows: 2983905, total_rows: 37100304\n",
      "started processing file: 000000175697\n",
      "wrote data from 23 file: 000000175697, rows: 3510655, total_rows: 40610959\n",
      "started processing file: 000000175703\n",
      "wrote data from 24 file: 000000175703, rows: 3306051, total_rows: 43917010\n",
      "started processing file: 000000175774\n",
      "wrote data from 25 file: 000000175774, rows: 3218001, total_rows: 47135011\n",
      "started processing file: 000000176016\n",
      "wrote data from 26 file: 000000176016, rows: 3211764, total_rows: 50346775\n",
      "started processing file: 000000176182\n",
      "wrote data from 27 file: 000000176182, rows: 179547, total_rows: 50526322\n",
      "started processing file: 000000176061\n",
      "wrote data from 28 file: 000000176061, rows: 3569461, total_rows: 54095783\n",
      "started processing file: 000000175920\n",
      "wrote data from 29 file: 000000175920, rows: 3654685, total_rows: 57750468\n",
      "started processing file: 000000175957\n",
      "wrote data from 30 file: 000000175957, rows: 3384195, total_rows: 61134663\n",
      "started processing file: 000000175594\n",
      "wrote data from 31 file: 000000175594, rows: 3255001, total_rows: 64389664\n",
      "started processing file: 000000175850\n",
      "wrote data from 32 file: 000000175850, rows: 3340243, total_rows: 67729907\n",
      "started processing file: 000000175827\n",
      "wrote data from 33 file: 000000175827, rows: 3095596, total_rows: 70825503\n",
      "started processing file: 000000176166\n",
      "wrote data from 34 file: 000000176166, rows: 181919, total_rows: 71007422\n",
      "started processing file: 000000176085\n",
      "wrote data from 35 file: 000000176085, rows: 201150, total_rows: 71208572\n",
      "started processing file: 000000176111\n",
      "wrote data from 36 file: 000000176111, rows: 819723, total_rows: 72028295\n",
      "started processing file: 000000176188\n",
      "wrote data from 37 file: 000000176188, rows: 219651, total_rows: 72247946\n",
      "started processing file: 000000176257\n",
      "wrote data from 38 file: 000000176257, rows: 178691, total_rows: 72426637\n",
      "started processing file: 000000176220\n",
      "wrote data from 39 file: 000000176220, rows: 171362, total_rows: 72597999\n",
      "started processing file: 000000175636\n",
      "wrote data from 40 file: 000000175636, rows: 3416641, total_rows: 76014640\n",
      "started processing file: 000000175641\n",
      "wrote data from 41 file: 000000175641, rows: 2757701, total_rows: 78772341\n",
      "started processing file: 000000175746\n",
      "wrote data from 42 file: 000000175746, rows: 691, total_rows: 78773032\n",
      "started processing file: 000000175731\n",
      "wrote data from 43 file: 000000175731, rows: 727, total_rows: 78773759\n",
      "started processing file: 000000176327\n",
      "wrote data from 44 file: 000000176327, rows: 180866, total_rows: 78954625\n",
      "started processing file: 000000176350\n",
      "wrote data from 45 file: 000000176350, rows: 146071, total_rows: 79100696\n",
      "started processing file: 000000175965\n",
      "wrote data from 46 file: 000000175965, rows: 3602298, total_rows: 82702994\n",
      "started processing file: 000000175912\n",
      "wrote data from 47 file: 000000175912, rows: 4332864, total_rows: 87035858\n",
      "started processing file: 000000175886\n",
      "wrote data from 48 file: 000000175886, rows: 3153947, total_rows: 90189805\n",
      "started processing file: 000000175868\n",
      "wrote data from 49 file: 000000175868, rows: 3931895, total_rows: 94121700\n",
      "started processing file: 000000176129\n",
      "wrote data from 50 file: 000000176129, rows: 137373, total_rows: 94259073\n",
      "started processing file: 000000176053\n",
      "wrote data from 51 file: 000000176053, rows: 3172889, total_rows: 97431962\n",
      "started processing file: 000000176024\n",
      "wrote data from 52 file: 000000176024, rows: 3450487, total_rows: 100882449\n",
      "started processing file: 000000176123\n",
      "wrote data from 53 file: 000000176123, rows: 179128, total_rows: 101061577\n",
      "started processing file: 000000176154\n",
      "wrote data from 54 file: 000000176154, rows: 168450, total_rows: 101230027\n",
      "started processing file: 000000176059\n",
      "wrote data from 55 file: 000000176059, rows: 3627884, total_rows: 104857911\n",
      "started processing file: 000000175918\n",
      "wrote data from 56 file: 000000175918, rows: 3776618, total_rows: 108634529\n",
      "started processing file: 000000175981\n",
      "wrote data from 57 file: 000000175981, rows: 3249379, total_rows: 111883908\n",
      "started processing file: 000000175815\n",
      "wrote data from 58 file: 000000175815, rows: 3781410, total_rows: 115665318\n",
      "started processing file: 000000175862\n",
      "wrote data from 59 file: 000000175862, rows: 3766147, total_rows: 119431465\n",
      "started processing file: 000000176413\n",
      "wrote data from 60 file: 000000176413, rows: 167612, total_rows: 119599077\n",
      "started processing file: 000000175818\n",
      "wrote data from 61 file: 000000175818, rows: 556017, total_rows: 120155094\n",
      "started processing file: 000000175881\n",
      "wrote data from 62 file: 000000175881, rows: 3364902, total_rows: 123519996\n",
      "started processing file: 000000175915\n",
      "wrote data from 63 file: 000000175915, rows: 3674566, total_rows: 127194562\n",
      "started processing file: 000000175962\n",
      "wrote data from 64 file: 000000175962, rows: 3398951, total_rows: 130593513\n",
      "started processing file: 000000176023\n",
      "wrote data from 65 file: 000000176023, rows: 4259184, total_rows: 134852697\n",
      "started processing file: 000000176054\n",
      "wrote data from 66 file: 000000176054, rows: 4018716, total_rows: 138871413\n",
      "started processing file: 000000176159\n",
      "wrote data from 67 file: 000000176159, rows: 96874, total_rows: 138968287\n",
      "started processing file: 000000176029\n",
      "wrote data from 68 file: 000000176029, rows: 3620806, total_rows: 142589093\n",
      "started processing file: 000000176153\n",
      "wrote data from 69 file: 000000176153, rows: 219024, total_rows: 142808117\n",
      "started processing file: 000000176124\n",
      "wrote data from 70 file: 000000176124, rows: 171281, total_rows: 142979398\n",
      "started processing file: 000000175865\n",
      "wrote data from 71 file: 000000175865, rows: 3574244, total_rows: 146553642\n",
      "started processing file: 000000175812\n",
      "wrote data from 72 file: 000000175812, rows: 4025730, total_rows: 150579372\n",
      "started processing file: 000000175986\n",
      "wrote data from 73 file: 000000175986, rows: 3033609, total_rows: 153612981\n",
      "started processing file: 000000175968\n",
      "wrote data from 74 file: 000000175968, rows: 3083537, total_rows: 156696518\n",
      "started processing file: 000000176227\n",
      "wrote data from 75 file: 000000176227, rows: 299573, total_rows: 156996091\n",
      "started processing file: 000000176250\n",
      "wrote data from 76 file: 000000176250, rows: 267129, total_rows: 157263220\n",
      "started processing file: 000000175646\n",
      "wrote data from 77 file: 000000175646, rows: 3019635, total_rows: 160282855\n",
      "started processing file: 000000175631\n",
      "wrote data from 78 file: 000000175631, rows: 2908300, total_rows: 163191155\n",
      "started processing file: 000000175736\n",
      "wrote data from 79 file: 000000175736, rows: 954, total_rows: 163192109\n",
      "started processing file: 000000175741\n",
      "wrote data from 80 file: 000000175741, rows: 596, total_rows: 163192705\n",
      "started processing file: 000000176357\n",
      "wrote data from 81 file: 000000176357, rows: 138391, total_rows: 163331096\n",
      "started processing file: 000000176320\n",
      "wrote data from 82 file: 000000176320, rows: 146006, total_rows: 163477102\n",
      "started processing file: 000000176066\n",
      "wrote data from 83 file: 000000176066, rows: 2981942, total_rows: 166459044\n",
      "started processing file: 000000176185\n",
      "wrote data from 84 file: 000000176185, rows: 139860, total_rows: 166598904\n",
      "started processing file: 000000176011\n",
      "wrote data from 85 file: 000000176011, rows: 3777554, total_rows: 170376458\n",
      "started processing file: 000000176088\n",
      "wrote data from 86 file: 000000176088, rows: 189303, total_rows: 170565761\n",
      "started processing file: 000000175593\n",
      "wrote data from 87 file: 000000175593, rows: 3771725, total_rows: 174337486\n",
      "started processing file: 000000175950\n",
      "wrote data from 88 file: 000000175950, rows: 2896282, total_rows: 177233768\n",
      "started processing file: 000000175927\n",
      "wrote data from 89 file: 000000175927, rows: 3464216, total_rows: 180697984\n",
      "started processing file: 000000175820\n",
      "wrote data from 90 file: 000000175820, rows: 3394013, total_rows: 184091997\n",
      "started processing file: 000000175857\n",
      "wrote data from 91 file: 000000175857, rows: 3901326, total_rows: 187993323\n",
      "started processing file: 000000175599\n",
      "wrote data from 92 file: 000000175599, rows: 4132480, total_rows: 192125803\n",
      "started processing file: 000000175577\n",
      "wrote data from 93 file: 000000175577, rows: 3020355, total_rows: 195146158\n",
      "started processing file: 000000176116\n",
      "wrote data from 94 file: 000000176116, rows: 311105, total_rows: 195457263\n",
      "started processing file: 000000176082\n",
      "wrote data from 95 file: 000000176082, rows: 184215, total_rows: 195641478\n",
      "started processing file: 000000176161\n",
      "wrote data from 96 file: 000000176161, rows: 199943, total_rows: 195841421\n",
      "started processing file: 000000175779\n",
      "wrote data from 97 file: 000000175779, rows: 3192511, total_rows: 199033932\n",
      "started processing file: 000000175797\n",
      "wrote data from 98 file: 000000175797, rows: 3235728, total_rows: 202269660\n",
      "started processing file: 000000175603\n",
      "wrote data from 99 file: 000000175603, rows: 3725623, total_rows: 205995283\n",
      "started processing file: 000000175674\n",
      "wrote data from 100 file: 000000175674, rows: 3550438, total_rows: 209545721\n",
      "started processing file: 000000176318\n",
      "wrote data from 101 file: 000000176318, rows: 152754, total_rows: 209698475\n",
      "started processing file: 000000176262\n",
      "wrote data from 102 file: 000000176262, rows: 458275, total_rows: 210156750\n",
      "started processing file: 000000176381\n",
      "wrote data from 103 file: 000000176381, rows: 163393, total_rows: 210320143\n",
      "started processing file: 000000176215\n",
      "wrote data from 104 file: 000000176215, rows: 297527, total_rows: 210617670\n",
      "started processing file: 000000176312\n",
      "wrote data from 105 file: 000000176312, rows: 143882, total_rows: 210761552\n",
      "started processing file: 000000176286\n",
      "wrote data from 106 file: 000000176286, rows: 76993, total_rows: 210838545\n",
      "started processing file: 000000176365\n",
      "wrote data from 107 file: 000000176365, rows: 205657, total_rows: 211044202\n",
      "started processing file: 000000176268\n",
      "wrote data from 108 file: 000000176268, rows: 157304, total_rows: 211201506\n",
      "started processing file: 000000175773\n",
      "wrote data from 109 file: 000000175773, rows: 2720531, total_rows: 213922037\n",
      "started processing file: 000000175704\n",
      "wrote data from 110 file: 000000175704, rows: 3048279, total_rows: 216970316\n",
      "started processing file: 000000175690\n",
      "wrote data from 111 file: 000000175690, rows: 219071, total_rows: 217189387\n",
      "started processing file: 000000175609\n",
      "wrote data from 112 file: 000000175609, rows: 2963000, total_rows: 220152387\n",
      "started processing file: 000000175898\n",
      "wrote data from 113 file: 000000175898, rows: 4731105, total_rows: 224883492\n",
      "started processing file: 000000175876\n",
      "wrote data from 114 file: 000000175876, rows: 3744902, total_rows: 228628394\n",
      "started processing file: 000000175801\n",
      "wrote data from 115 file: 000000175801, rows: 3153746, total_rows: 231782140\n",
      "started processing file: 000000175995\n",
      "wrote data from 116 file: 000000175995, rows: 3176312, total_rows: 234958452\n",
      "started processing file: 000000176140\n",
      "wrote data from 117 file: 000000176140, rows: 220817, total_rows: 235179269\n",
      "started processing file: 000000176137\n",
      "wrote data from 118 file: 000000176137, rows: 119260, total_rows: 235298529\n",
      "started processing file: 000000176030\n",
      "wrote data from 119 file: 000000176030, rows: 3806044, total_rows: 239104573\n",
      "started processing file: 000000176047\n",
      "wrote data from 120 file: 000000176047, rows: 3229234, total_rows: 242333807\n",
      "started processing file: 000000176400\n",
      "wrote data from 121 file: 000000176400, rows: 134562, total_rows: 242468369\n",
      "started processing file: 000000175892\n",
      "wrote data from 122 file: 000000175892, rows: 3524798, total_rows: 245993167\n",
      "started processing file: 000000175906\n",
      "wrote data from 123 file: 000000175906, rows: 3376482, total_rows: 249369649\n",
      "started processing file: 000000175971\n",
      "wrote data from 124 file: 000000175971, rows: 2793292, total_rows: 252162941\n",
      "started processing file: 000000176249\n",
      "wrote data from 125 file: 000000176249, rows: 153230, total_rows: 252316171\n",
      "started processing file: 000000176344\n",
      "wrote data from 126 file: 000000176344, rows: 15937, total_rows: 252332108\n",
      "started processing file: 000000176333\n",
      "wrote data from 127 file: 000000176333, rows: 146282, total_rows: 252478390\n",
      "started processing file: 000000175628\n",
      "wrote data from 128 file: 000000175628, rows: 1748, total_rows: 252480138\n",
      "started processing file: 000000175725\n",
      "wrote data from 129 file: 000000175725, rows: 3233050, total_rows: 255713188\n",
      "started processing file: 000000175752\n",
      "wrote data from 130 file: 000000175752, rows: 763, total_rows: 255713951\n",
      "started processing file: 000000175655\n",
      "wrote data from 131 file: 000000175655, rows: 3567072, total_rows: 259281023\n",
      "started processing file: 000000175622\n",
      "wrote data from 132 file: 000000175622, rows: 1582, total_rows: 259282605\n",
      "started processing file: 000000175758\n",
      "wrote data from 133 file: 000000175758, rows: 1600365, total_rows: 260882970\n",
      "started processing file: 000000176234\n",
      "wrote data from 134 file: 000000176234, rows: 150727, total_rows: 261033697\n",
      "started processing file: 000000176243\n",
      "wrote data from 135 file: 000000176243, rows: 50457, total_rows: 261084154\n",
      "started processing file: 000000176339\n",
      "wrote data from 136 file: 000000176339, rows: 163648, total_rows: 261247802\n",
      "started processing file: 000000176105\n",
      "wrote data from 137 file: 000000176105, rows: 858354, total_rows: 262106156\n",
      "started processing file: 000000176091\n",
      "wrote data from 138 file: 000000176091, rows: 136010, total_rows: 262242166\n",
      "started processing file: 000000176172\n",
      "wrote data from 139 file: 000000176172, rows: 112252, total_rows: 262354418\n",
      "started processing file: 000000176008\n",
      "wrote data from 140 file: 000000176008, rows: 3461939, total_rows: 265816357\n",
      "started processing file: 000000175949\n",
      "wrote data from 141 file: 000000175949, rows: 4266513, total_rows: 270082870\n",
      "started processing file: 000000175833\n",
      "wrote data from 142 file: 000000175833, rows: 4346504, total_rows: 274429374\n",
      "started processing file: 000000175844\n",
      "wrote data from 143 file: 000000175844, rows: 4284013, total_rows: 278713387\n",
      "started processing file: 000000175943\n",
      "wrote data from 144 file: 000000175943, rows: 3556813, total_rows: 282270200\n",
      "started processing file: 000000175934\n",
      "wrote data from 145 file: 000000175934, rows: 3226873, total_rows: 285497073\n",
      "started processing file: 000000175580\n",
      "wrote data from 146 file: 000000175580, rows: 2823617, total_rows: 288320690\n",
      "started processing file: 000000175839\n",
      "wrote data from 147 file: 000000175839, rows: 4478063, total_rows: 292798753\n",
      "started processing file: 000000176178\n",
      "wrote data from 148 file: 000000176178, rows: 45615, total_rows: 292844368\n",
      "started processing file: 000000176075\n",
      "wrote data from 149 file: 000000176075, rows: 137310, total_rows: 292981678\n",
      "started processing file: 000000176196\n",
      "wrote data from 150 file: 000000176196, rows: 201815, total_rows: 293183493\n",
      "started processing file: 000000176002\n",
      "wrote data from 151 file: 000000176002, rows: 3883961, total_rows: 297067454\n",
      "started processing file: 000000175760\n",
      "wrote data from 152 file: 000000175760, rows: 3080358, total_rows: 300147812\n",
      "started processing file: 000000175717\n",
      "wrote data from 153 file: 000000175717, rows: 3326290, total_rows: 303474102\n",
      "started processing file: 000000175683\n",
      "wrote data from 154 file: 000000175683, rows: 2619420, total_rows: 306093522\n",
      "started processing file: 000000176398\n",
      "wrote data from 155 file: 000000176398, rows: 127902, total_rows: 306221424\n",
      "started processing file: 000000176301\n",
      "wrote data from 156 file: 000000176301, rows: 135065, total_rows: 306356489\n",
      "started processing file: 000000176295\n",
      "wrote data from 157 file: 000000176295, rows: 146945, total_rows: 306503434\n",
      "started processing file: 000000176376\n",
      "wrote data from 158 file: 000000176376, rows: 188198, total_rows: 306691632\n",
      "started processing file: 000000176271\n",
      "wrote data from 159 file: 000000176271, rows: 13559, total_rows: 306705191\n",
      "started processing file: 000000176392\n",
      "wrote data from 160 file: 000000176392, rows: 216515, total_rows: 306921706\n",
      "started processing file: 000000176206\n",
      "wrote data from 161 file: 000000176206, rows: 153012, total_rows: 307074718\n",
      "started processing file: 000000175784\n",
      "wrote data from 162 file: 000000175784, rows: 2850792, total_rows: 309925510\n",
      "started processing file: 000000175610\n",
      "wrote data from 163 file: 000000175610, rows: 2742495, total_rows: 312668005\n",
      "started processing file: 000000175667\n",
      "wrote data from 164 file: 000000175667, rows: 3179465, total_rows: 315847470\n",
      "started processing file: 000000175689\n",
      "wrote data from 165 file: 000000175689, rows: 2032469, total_rows: 317879939\n",
      "total number of records: 317879939\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import text\n",
    "def relations_populate_postgres_avro():\n",
    "    # engine = sqlalchemy.create_engine(\"postgresql+psycopg://{user}:{pw}@{host}:{port}/{db}\"\n",
    "    #                    .format(user=\"metricsuser\",\n",
    "    #                            pw=\"metricspassword\",\n",
    "    #                            host=\"localhost\",\n",
    "    #                            port=\"5432\",\n",
    "    #                            db=\"metrics\"))\n",
    "    connection_str = \"postgresql{dbapi}://{user}:{pw}@{host}:{port}/{db}\".format(\n",
    "        dbapi='',\n",
    "        # dbapi=\"+pg8000\",\n",
    "        # dbapi=\"+psycopg\",\n",
    "        user=\"metricsuser\",\n",
    "        pw=\"metricspassword\",\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\",\n",
    "        db=\"metrics\")\n",
    "    # print (connection_str)\n",
    "    engine = sqlalchemy.create_engine(connection_str)\n",
    "    with engine.begin() as connection:\n",
    "        print (connection)\n",
    "        # Execute a query\n",
    "        connection.exec_driver_sql(\"\"\"CREATE TABLE IF NOT EXISTS relations (\n",
    "                                    system_name VARCHAR,\n",
    "                                    from_package_name VARCHAR,\n",
    "                                    from_version VARCHAR,\n",
    "                                    to_package_name VARCHAR,\n",
    "                                    actual_requirement VARCHAR,\n",
    "                                    to_version VARCHAR,\n",
    "                                    to_package_highest_available_release VARCHAR,\n",
    "                                    interval_start TIMESTAMP,\n",
    "                                    interval_end TIMESTAMP,\n",
    "                                    is_out_of_date BOOLEAN,\n",
    "                                    is_regular BOOLEAN\n",
    "                                );\"\"\")\n",
    "        # print (result)\n",
    "        \n",
    "        # # Fetch and display the result\n",
    "        # db_version = result.fetchone()\n",
    "        # print(f\"Database version: {db_version[0]}\")\n",
    "\n",
    "    data_dir = os.path.join(\"../../../\", \"durations-data/20240821-avro/\")\n",
    "    total_number_of_rows = 0\n",
    "    number_of_file_processed = 0\n",
    "    for file in os.listdir(data_dir):\n",
    "        file_path = os.path.join(data_dir, file)\n",
    "\n",
    "        if check_if_this_file_has_been_processed(file):\n",
    "            print (f\"file {file} has already been processed\")\n",
    "            continue\n",
    "\n",
    "        df = avro_df(file_path, 'rb')\n",
    "        if (df.shape[0] == 0):\n",
    "            continue\n",
    "\n",
    "        print(f\"started processing file: {file}\")\n",
    "\n",
    "        # schema, records = read_avro(file_path)\n",
    "        # print (schema)\n",
    "        # if len(records) > 0:\n",
    "        #     print (records[0])\n",
    "        #     break\n",
    "        # # System;Name;Version;RequirementName;RequirementVersion;HighestMatchingVersion;HighestAvailableRelease;IntervalStart;IntervalEnd;IsOutOfDate;IsRegular;SnapshotAt;Warnings\n",
    "        \n",
    "        # Change the name of the columns for our ease\n",
    "        df.columns = ['system_name', 'from_package_name', 'from_version', 'to_package_name', 'actual_requirement', 'to_version', 'to_package_highest_available_release', 'interval_start', 'interval_end', 'is_out_of_date', 'is_regular', 'warnings']\n",
    "\n",
    "        # Drop the 'warnings' column\n",
    "        df.drop('warnings', axis=1, inplace=True)\n",
    "\n",
    "        # We don't want to drop rows having NULL values in any column.\n",
    "        # df = df.dropna()\n",
    "        df.dropna(subset=['system_name', 'from_package_name', 'from_version', 'to_package_name', 'to_version'], inplace=True)\n",
    "        # Dropping duplicates is super necessary for us since the dataset contains the same relations over and over again because of the resolution of transitive dependencies also.\n",
    "        df.drop_duplicates(subset=['system_name', 'from_package_name', 'from_version', 'to_package_name', 'actual_requirement', 'to_version', 'to_package_highest_available_release', 'interval_start', 'interval_end', 'is_out_of_date', 'is_regular'], inplace=True) # Commenting since we got the error : unhashable type 'list'\n",
    "\n",
    "        \n",
    "        try:\n",
    "            with engine.begin() as connection:\n",
    "                # print (connection)\n",
    "\n",
    "                df.to_sql(\n",
    "                    name='relations',\n",
    "                    con=connection,\n",
    "                    if_exists='append',\n",
    "                    index=False,\n",
    "                    method=psql_insert_copy # https://stackoverflow.com/a/55495065/3450691\n",
    "                )\n",
    "                rows = df.shape[0]\n",
    "                total_number_of_rows += rows\n",
    "                number_of_file_processed += 1\n",
    "                print(f\"wrote data from {number_of_file_processed} file: {file}, rows: {rows}, total_rows: {total_number_of_rows}\")\n",
    "                mark_this_file_as_processed(file)\n",
    "        except psycopg2.OperationalError as e:\n",
    "            print(\"Query canceled:\", e)\n",
    "            mark_this_file_as_error(file)\n",
    "\n",
    "            # df.to_sql(\n",
    "            #         # con=engine,\n",
    "            #         con=connection,\n",
    "            #         name='relations',\n",
    "            #         if_exists='append',\n",
    "            #         index=False,\n",
    "            #         method='multi',\n",
    "            #         # chunksize=chunk_size,\n",
    "            #         dtype={\n",
    "            #                 'system_name': sqlalchemy.types.VARCHAR,\n",
    "            #                 'from_package_name': sqlalchemy.types.VARCHAR,\n",
    "            #                 'from_version': sqlalchemy.types.VARCHAR,\n",
    "            #                 'to_package_name': sqlalchemy.types.VARCHAR,\n",
    "            #                 'actual_requirement': sqlalchemy.types.VARCHAR,\n",
    "            #                 'to_version': sqlalchemy.types.VARCHAR,\n",
    "            #                 'to_package_highest_available_release': sqlalchemy.types.VARCHAR,\n",
    "            #                 'interval_start': sqlalchemy.types.TIMESTAMP,\n",
    "            #                 'interval_end': sqlalchemy.types.TIMESTAMP,\n",
    "            #                 'is_out_of_date': sqlalchemy.types.BOOLEAN,\n",
    "            #                 'is_regular': sqlalchemy.types.BOOLEAN\n",
    "            #                 # 'warnings': sqlalchemy.types.VARCHAR\n",
    "            #             })\n",
    "        \n",
    "    print (f\"total number of records: {total_number_of_rows}\")\n",
    "\n",
    "relations_populate_postgres_avro()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: add the avro data files for which we got the OperationalError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimeOutOfDate populate in POSTGRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_out_of_date_populate_postgres():\n",
    "    engine = sqlalchemy.create_engine(\"postgresql+psycopg://{user}:{pw}@localhost/{db}\"\n",
    "                       .format(user=\"postgres\",\n",
    "                               pw=\"shimul\",\n",
    "                               db=\"postgres\"))\n",
    "    data_dir = os.path.join(os.path.join(os.path.join(os.path.join(os.path.join(os.getcwd(), os.pardir), os.pardir), os.pardir), \"data-shared-from-deps-dev\"), \"time-out-of-date\")\n",
    "    print (data_dir)\n",
    "    total_number_of_rows = 0\n",
    "    for csv_file in os.listdir(data_dir):\n",
    "        df = pd.read_csv(os.path.join(data_dir, csv_file),\n",
    "                        header=0,\n",
    "                        sep=';',\n",
    "                        on_bad_lines='skip',\n",
    "                        usecols=['System', 'Name', 'TotalRequirementDuration', 'OutOfDateDuration', 'Ratio', 'DependentsApprox'],\n",
    "                        )\n",
    "        df.columns = ['system_name', 'package_name', 'total_requirement_duration', 'out_of_date_duration', 'ratio', 'dependents_approx']\n",
    "        \n",
    "        with engine.begin() as connection:\n",
    "            df.to_sql(con=connection,\n",
    "                      name='out_of_date_duration_google',\n",
    "                      if_exists='append',\n",
    "                      index=False,\n",
    "                      dtype={\n",
    "                                'system_name': sqlalchemy.types.VARCHAR,\n",
    "                                'package_name': sqlalchemy.types.VARCHAR,\n",
    "                                'total_requirement_duration': sqlalchemy.types.INTEGER,\n",
    "                                'out_of_date_duration': sqlalchemy.types.INTEGER,\n",
    "                                'ratio': sqlalchemy.types.FLOAT,\n",
    "                                'dependents_approx': sqlalchemy.types.INTEGER\n",
    "                            })\n",
    "        rows = df.shape[0]\n",
    "        total_number_of_rows += rows\n",
    "        print(f\"wrote data from file: {csv_file}, rows: {rows}, total_rows: {total_number_of_rows}\")\n",
    "\n",
    "time_out_of_date_populate_postgres()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move the versioninfo data to postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../data/cargo-pkgver\"\n",
    "df = pd.read_pickle(os.path.join(data_dir, 'all.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['system_name', 'package_name', 'version_name', 'release_date'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def versioninfo_populate_postgres(data_dir):\n",
    "    df = pd.read_pickle(os.path.join(data_dir, 'all.pkl'))\n",
    "    connection_str = \"postgresql{dbapi}://{user}:{pw}@{host}:{port}/{db}\".format(\n",
    "            dbapi='',\n",
    "            # dbapi=\"+pg8000\",\n",
    "            # dbapi=\"+psycopg\",\n",
    "            user=\"metricsuser\",\n",
    "            pw=\"metricspassword\",\n",
    "            host=\"localhost\",\n",
    "            port=\"5432\",\n",
    "            db=\"metrics\")\n",
    "        # print (connection_str)\n",
    "    engine = sqlalchemy.create_engine(connection_str)\n",
    "    with engine.begin() as connection:\n",
    "        df.to_sql(\n",
    "            name='versioninfo',\n",
    "            con=connection,\n",
    "            if_exists='append',\n",
    "            index=False,\n",
    "            dtype={\n",
    "                    'system_name': sqlalchemy.types.VARCHAR,\n",
    "                    'package_name': sqlalchemy.types.VARCHAR,\n",
    "                    'version_name': sqlalchemy.types.VARCHAR,\n",
    "                    'release_date': sqlalchemy.types.TIMESTAMP\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "versioninfo_populate_postgres(\"../../data/cargo-pkgver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "versioninfo_populate_postgres(\"../../data/npm-pkgver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "versioninfo_populate_postgres(\"../../data/pypi-pkgver\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "secmet_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
